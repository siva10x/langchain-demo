{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain Document Loaders Overview\n",
    "\n",
    "This notebook demonstrates how to ingest data from various sources using LangChain's document loaders, including text files, PDFs, webpages, arXiv research papers, and Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Document Loaders Covered\n",
    "\n",
    "- TextLoader (plain text files)\n",
    "- PyPDFLoader (PDF files)\n",
    "- WebBaseLoader (webpages)\n",
    "- ArxivLoader (arXiv research papers)\n",
    "- WikipediaLoader (Wikipedia articles)\n",
    "\n",
    "Each section will show how to ingest data from the respective source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, install all required packages\n",
    "# !pip install langchain openai python-dotenv pypdf requests beautifulsoup4 arxiv wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Ingesting Text Files with TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install for TextLoader (text files)\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Create a sample text file\n",
    "sample_text_path = \"./temp/sample_data.txt\"\n",
    "with open(sample_text_path, \"w\") as f:\n",
    "    f.write(\"LangChain makes it easy to work with language models and ingest data from various sources.\")\n",
    "\n",
    "# Load the text file using LangChain\n",
    "loader = TextLoader(sample_text_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Loaded documents:\")\n",
    "for doc in documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Ingesting PDF Files with PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install for PyPDFLoader (PDF files)\n",
    "# !pip install langchain pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load a sample PDF file (replace with your own PDF path if needed)\n",
    "# For demonstration, this cell assumes sample.pdf exists in temp folder.\n",
    "pdf_path = \"./temp/sample.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from PDF.\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"--- Page {i+1} ---\")\n",
    "    print(doc.page_content[:500])  # Print first 500 characters for brevity\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Ingesting Webpages with WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install for WebBaseLoader (webpages)\n",
    "# !pip install langchain requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# Specify the URL of the webpage to ingest\n",
    "url = \"https://www.geeksforgeeks.org/artificial-intelligence/what-is-generative-ai/\"\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from the webpage.\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:500])  # Print first 500 characters for brevity\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4. Ingesting arXiv Research Papers with ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install for ArxivLoader (arXiv research papers)\n",
    "# !pip install langchain arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are getting SSLCertVerificationError, you can try disabling SSL verification (not recommended for production)\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "# The arXiv ID for 'Attention Is All You Need' is 1706.03762\n",
    "arxiv_id = \"1706.03762\"\n",
    "loader = ArxivLoader(arxiv_id)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from arXiv.\")\n",
    "if documents:\n",
    "    print(\"Title:\", documents[0].metadata.get('Title', 'N/A'))\n",
    "    print(\"\\nAbstract/Content:\\n\", documents[0].page_content[:1000])  # Print first 1000 chars\n",
    "else:\n",
    "    print(\"No documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 5. Ingesting Wikipedia Articles with WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install for WikipediaLoader (Wikipedia articles)\n",
    "# !pip install langchain wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# Specify the Wikipedia page to ingest\n",
    "page_title = \"Transformer (machine learning model)\"\n",
    "loader = WikipediaLoader(query=page_title, lang=\"en\", load_max_docs=2)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from Wikipedia.\")\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"Title: {doc.metadata['title']}\")\n",
    "    print(f\"URL: {doc.metadata['source']}\")\n",
    "    print()\n",
    "\n",
    "# Print the content of the loaded Wikipedia article\n",
    "if documents:\n",
    "    print(documents[0].page_content[:1000])  # Print first 1000 characters\n",
    "else:\n",
    "    print(\"No documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You can now use these loaded documents for further processing, such as splitting, embedding, or querying with LangChain tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
